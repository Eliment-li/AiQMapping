
env_version: 11
reward_function_version: 3
circuit_path: data/circuits/xeb/xeb5
###Iters is the number of batches the model will train on and the number of times your model weights will be updated (not counting minibatches).
stop_iters: 2

#One call to env.step() is one timestep.
stop_timesteps: 99999999

#the reward for multi-agent is the total sum (not the mean) over the agents.
stop_reward: 100

no_tune: False
local_mode: False
framework: torch
checkpoint_frequency: 10
checkpoint_at_end: True
lr: 1e-4
qasm: None
log_file_id: 0
gamma: 0.99
#fcnet_hiddens: [1024,2048,2048,1024,512,256]
#fcnet_hiddens: [32,64,64,32,16]
fcnet_activation: Swish
run: PPO
#在config.py中有自动配置
num_gpus: 1
#resume: False
checkpoint: None

#the save path of check_point zip file
check_point_zip_path: None

debug: False


explore_during_inference: False
#attention start
use_attention: True

# The number of transformer units within GTrXL.
# A transformer unit in GTrXL consists of :
# a) MultiHeadAttention module and
# b) a position-wise MLP.
attention_num_transformer_units: 1

# The number of attention heads within the MultiHeadAttention units.
attention_num_heads: 1,
# The dim of a single head (within the MultiHeadAttention units).
attention_head_dim: 32,

# Whether to feed a_{t-n:t-1} to GTrXL (one-hot encoded if discrete).
prev_n_actions: 0
# Whether to feed r_{t-n:t-1} to GTrXL.
prev_n_rewards: 0
 # The input and output size of each transformer unit.
attention_dim: 32

# The memory sizes for inference and training.
attention_memory_inference: 10
attention_memory_training: 10

# The output dim of the position-wise MLP.
attention_position_wise_mlp_dim: 32
# The initial bias values for the 2 GRU gates within a transformer unit.
attention_init_gru_gate_bias: 2.0

#attention end



#unimportant
show_trace: False
storage_path: d:/file/ray_results