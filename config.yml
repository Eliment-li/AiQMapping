
env_version: 8
reward_function_version: 1
circuit_path: data/circuits/xeb/xeb5
###Iters is the number of batches the model will train on and the number of times your model weights will be updated (not counting minibatches).
stop_iters: 2

#One call to env.step() is one timestep.
stop_timesteps: 99999999

#the reward for multi-agent is the total sum (not the mean) over the agents.
stop_reward: 100

no_tune: False
local_mode: False
framework: torch
checkpoint_frequency: 2
checkpoint_at_end: True
lr: 1e-4
qasm: None
log_file_id: 0
gamma: 0.99
#fcnet_hiddens: [1024,2048,2048,1024,512,256]
#fcnet_hiddens: [32,64,64,32,16]
fcnet_activation: Swish
run: PPO
num_gpus: 1
#resume: False
checkpoint: None

#the save path of check_point zip file
check_point_zip_path: None

debug: False

explore_during_inference: False

#attention:
use_attention: False
prev-n-actions: 0
prev-n-rewards: 0

attention_num_transformer_units: 1
attention_dim: 32,
attention_memory_inference: 10,
attention_memory_training: 10,
#rllib ppo end

#unimportant
show_trace: False